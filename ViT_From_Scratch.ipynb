{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from torchsummary import summary    \n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomHorizontalFlip, RandomCrop\n",
    "\n",
    "\n",
    "\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "# to make progrss bar work\n",
    "!jupyter nbextension enable --py widgetsnbextension   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "patch_size=16\n",
    "latent_size=768 \n",
    "n_channels = 3\n",
    "num_heads = 12\n",
    "num_encoders = 12\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "size = 224\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 0.001\n",
    "weight_decay = 0.03\n",
    "batch_size = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, device=device, patch_size=patch_size, n_channels=n_channels, latent_size=latent_size,batch_size=batch_size):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "        self.device = device\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.latent_size = latent_size\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels #16*16*3\n",
    "        \n",
    "        #Linear Projection\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "        \n",
    "        #class token\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1,self.latent_size)).to(self.device)\n",
    "\n",
    "        # pos embedding\n",
    "        self.pos_embedding= nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self,input_data):\n",
    "        input_data = input_data.to(self.device)\n",
    "        \n",
    "        #patchify the image\n",
    "        patches = einops.rearrange(input_data, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        \"\"\"\n",
    "            Image is represented with 4 dimensions ( batch, channels, height, width ). Now height = x and patch_size similarly for width\n",
    "            So image dimensions are (batch, channels, height = x*patch_size, width = y*patch_size)\n",
    "            patches is represented with 3 dimensions ( batch, num_patches = x*y, channels*patch_size*patch_size )\n",
    "            For each patch we are going to apply linear projection\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Size of input data is :{input_data.size()}\")\n",
    "        print(f\"Size of patches is :{patches.size()}\")\n",
    "        \n",
    "        # Linear Projection\n",
    "        linear_projection = self.linearProjection(patches).to(self.device)\n",
    "        print(f\"Size of Linear Projection is :{linear_projection.size()}\")\n",
    "        b,n,_ = linear_projection.size()\n",
    "        # pre pend the class token to the linear projections\n",
    "        linear_projection = torch.cat([self.class_token, linear_projection], dim=1) # we are inserting across the patch dimension which is numbered one\n",
    "        print(f\"Size of linear projection with class token is :{linear_projection.size()}\")\n",
    "        \n",
    "        # add the positional embedding to all patches. so repeat pos embbedding == number of patches\n",
    "        pos_embeddings=einops.repeat(self.pos_embedding, 'b 1 d -> b m d',m=n+1)\n",
    "        print(f\"Size of positional embedding is :{self.pos_embedding.size()}\")\n",
    "        \n",
    "        # add the positional embedding to the linear projection\n",
    "        linear_projection = linear_projection + pos_embeddings\n",
    "        print(f\"Size of linear projection with positional embedding is :{linear_projection.size()}\")\n",
    "        \n",
    "        return linear_projection\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input data is :torch.Size([1, 3, 224, 224])\n",
      "Size of patches is :torch.Size([1, 196, 768])\n",
      "Size of Linear Projection is :torch.Size([1, 196, 768])\n",
      "Size of linear projection with class token is :torch.Size([1, 197, 768])\n",
      "Size of positional embedding is :torch.Size([1, 1, 768])\n",
      "Size of linear projection with positional embedding is :torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6310, -0.3207,  1.7034,  ...,  1.5155,  1.5132,  0.8933],\n",
       "         [ 0.1953,  1.3033,  0.5022,  ...,  0.8689, -0.1009,  1.1851],\n",
       "         [ 0.6021,  1.6609,  1.0280,  ..., -0.1888,  0.4152,  0.1821],\n",
       "         ...,\n",
       "         [ 1.0463,  0.5505,  1.1236,  ...,  1.0360, -0.0582,  0.5389],\n",
       "         [ 1.8190,  0.1885,  0.8947,  ...,  0.3344,  0.7989,  1.1417],\n",
       "         [-0.4104,  0.9053,  1.7362,  ...,  0.3034,  0.2745,  1.4105]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.randn((1,3,224,224))\n",
    "test_class = InputEmbedding().to(device)\n",
    "\n",
    "embedding_patches = test_class(test_data)\n",
    "embedding_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, device=device, latent_size=latent_size, num_heads=num_heads, dropout=dropout):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.device = device\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Normalization Layer\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        \n",
    "        # multi head attention\n",
    "        self.multihead = nn.MultiheadAttention(self.latent_size, self.num_heads,self.dropout)\n",
    "        \n",
    "        # FFL or MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size), \n",
    "            nn.Dropout(self.dropout)\n",
    "            )  \n",
    "    \n",
    "    def forward(self, embedded_patches):\n",
    "        # 1. Norm\n",
    "        firstNormOut = self.norm(embedded_patches)\n",
    "        # 2. multi head attention\n",
    "        attention_out = self.multihead(firstNormOut,firstNormOut,firstNormOut)[0] \n",
    "        # 3. first residual connection\n",
    "        firstResidual_out = attention_out+embedded_patches\n",
    "        # 4. second norm\n",
    "        secondNorm_out = self.norm(firstResidual_out)\n",
    "        # 5. mlp\n",
    "        mlp_out = self.mlp(secondNorm_out)\n",
    "        # 6. second residual connection\n",
    "        output = mlp_out + firstResidual_out\n",
    "        \n",
    "        print(f\"Embedded Pathces size is :{embedded_patches.size()}\")\n",
    "        print(f\"Output size is: {output.size()}\")   \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3052, -0.1552,  1.3714,  ...,  1.5523,  1.8341,  0.7944],\n",
       "         [ 0.0802,  0.3924,  1.1418,  ...,  0.8873,  0.1584,  2.0419],\n",
       "         [ 1.1035,  1.4225,  1.2099,  ...,  0.4294,  1.0284,  1.1855],\n",
       "         ...,\n",
       "         [ 1.4826,  0.3611,  0.9429,  ...,  1.2505, -0.1727,  1.3183],\n",
       "         [ 2.4398,  0.0811,  1.1461,  ...,  0.6752,  1.1753,  1.5884],\n",
       "         [-0.0375,  0.5218,  2.0174,  ...,  0.0156,  0.5107,  1.5428]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = EncoderBlock().to(device)\n",
    "test_encoder(embedding_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling a Vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,num_encoders = num_encoders, latent_size=latent_size,patch_size=patch_size,n_channels=n_channels,batch_size = batch_size, device = device, num_classes = num_classes, dropout=dropout, num_heads = num_heads):\n",
    "        super(ViT,self).__init__()\n",
    "        self.device = device\n",
    "        self.num_encoders = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # input embedding\n",
    "        self.inputEmbedding = InputEmbedding(self.device, self.patch_size, self.n_channels, self.latent_size,self.batch_size)\n",
    "        \n",
    "        # encoder\n",
    "        self.encStack = nn.ModuleList([EncoderBlock(self.device, self.latent_size, self.num_heads, self.dropout) for _ in range(self.num_encoders)])\n",
    "        \n",
    "        # MLP head\n",
    "        self.mlpHead=nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(  self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        input_data = input_data.to(self.device)\n",
    "        # get the embedding\n",
    "        embedding_out = self.inputEmbedding(input_data)\n",
    "        # pass embedding through encoder stack\n",
    "        for encoder in self.encStack:\n",
    "            embedding_out= encoder(embedding_out)\n",
    "        \n",
    "        #get the cls token\n",
    "        cls = embedding_out[:,0]\n",
    "        # pass through the mlp head\n",
    "        output = self.mlpHead(cls)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input data is :torch.Size([1, 3, 224, 224])\n",
      "Size of patches is :torch.Size([1, 196, 768])\n",
      "Size of Linear Projection is :torch.Size([1, 196, 768])\n",
      "Size of linear projection with class token is :torch.Size([1, 197, 768])\n",
      "Size of positional embedding is :torch.Size([1, 1, 768])\n",
      "Size of linear projection with positional embedding is :torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "Embedded Pathces size is :torch.Size([1, 197, 768])\n",
      "Output size is: torch.Size([1, 197, 768])\n",
      "tensor([[ 0.3207, -0.1317,  0.1351,  0.1210,  0.2612, -0.0267, -0.3147, -0.9537,\n",
      "          0.3620, -0.0190]], grad_fn=<AddmmBackward0>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_ViT = ViT().to(device)\n",
    "output=test_ViT(test_data)\n",
    "print(output,output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
